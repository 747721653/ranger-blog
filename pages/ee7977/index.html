<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer笔记 | 游侠的博客</title>
    <meta name="generator" content="VuePress 1.9.5">
    <link rel="icon" href="/ranger-blog/img/logo.png">
    <meta name="description" content="一名计算机专业研究生的博客&amp;学习笔记记录网站">
    <meta name="keywords" content="前端博客,个人技术博客,前端,前端开发,前端框架,web前端,前端面试题,技术文档,学习,面试,JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/ranger-blog/assets/css/0.styles.02a3a2c9.css" as="style"><link rel="preload" href="/ranger-blog/assets/js/app.cee5ce8e.js" as="script"><link rel="preload" href="/ranger-blog/assets/js/2.9c955742.js" as="script"><link rel="preload" href="/ranger-blog/assets/js/10.5d584cd8.js" as="script"><link rel="prefetch" href="/ranger-blog/assets/js/11.b85bbd57.js"><link rel="prefetch" href="/ranger-blog/assets/js/12.7fb04237.js"><link rel="prefetch" href="/ranger-blog/assets/js/13.96da4990.js"><link rel="prefetch" href="/ranger-blog/assets/js/14.7252c402.js"><link rel="prefetch" href="/ranger-blog/assets/js/15.82d03717.js"><link rel="prefetch" href="/ranger-blog/assets/js/16.f77b04cc.js"><link rel="prefetch" href="/ranger-blog/assets/js/17.5dfe6d16.js"><link rel="prefetch" href="/ranger-blog/assets/js/18.05143f46.js"><link rel="prefetch" href="/ranger-blog/assets/js/19.c2e9bd8f.js"><link rel="prefetch" href="/ranger-blog/assets/js/20.93056717.js"><link rel="prefetch" href="/ranger-blog/assets/js/21.ea301352.js"><link rel="prefetch" href="/ranger-blog/assets/js/22.173c77d8.js"><link rel="prefetch" href="/ranger-blog/assets/js/23.bba3f77e.js"><link rel="prefetch" href="/ranger-blog/assets/js/24.9ea69e66.js"><link rel="prefetch" href="/ranger-blog/assets/js/25.e18ccf89.js"><link rel="prefetch" href="/ranger-blog/assets/js/26.c686cf42.js"><link rel="prefetch" href="/ranger-blog/assets/js/27.174a6a49.js"><link rel="prefetch" href="/ranger-blog/assets/js/28.21b94394.js"><link rel="prefetch" href="/ranger-blog/assets/js/29.c439a3f5.js"><link rel="prefetch" href="/ranger-blog/assets/js/3.7fc9d9ba.js"><link rel="prefetch" href="/ranger-blog/assets/js/30.253fe47f.js"><link rel="prefetch" href="/ranger-blog/assets/js/31.ccb38dc7.js"><link rel="prefetch" href="/ranger-blog/assets/js/32.22b892aa.js"><link rel="prefetch" href="/ranger-blog/assets/js/33.ebe0152a.js"><link rel="prefetch" href="/ranger-blog/assets/js/34.78dd1485.js"><link rel="prefetch" href="/ranger-blog/assets/js/35.78695d18.js"><link rel="prefetch" href="/ranger-blog/assets/js/36.db982513.js"><link rel="prefetch" href="/ranger-blog/assets/js/37.6bbcacd5.js"><link rel="prefetch" href="/ranger-blog/assets/js/38.887ee71c.js"><link rel="prefetch" href="/ranger-blog/assets/js/39.4929761f.js"><link rel="prefetch" href="/ranger-blog/assets/js/4.80ba83f3.js"><link rel="prefetch" href="/ranger-blog/assets/js/40.b16edad6.js"><link rel="prefetch" href="/ranger-blog/assets/js/41.b858f05c.js"><link rel="prefetch" href="/ranger-blog/assets/js/42.36b49153.js"><link rel="prefetch" href="/ranger-blog/assets/js/43.b2351af9.js"><link rel="prefetch" href="/ranger-blog/assets/js/44.32a52b21.js"><link rel="prefetch" href="/ranger-blog/assets/js/45.4b402ffb.js"><link rel="prefetch" href="/ranger-blog/assets/js/46.7c223047.js"><link rel="prefetch" href="/ranger-blog/assets/js/47.60020f70.js"><link rel="prefetch" href="/ranger-blog/assets/js/48.8ff8ff56.js"><link rel="prefetch" href="/ranger-blog/assets/js/49.8c3c4b29.js"><link rel="prefetch" href="/ranger-blog/assets/js/5.928fd018.js"><link rel="prefetch" href="/ranger-blog/assets/js/50.275526a7.js"><link rel="prefetch" href="/ranger-blog/assets/js/51.39b3d03c.js"><link rel="prefetch" href="/ranger-blog/assets/js/52.83a1a466.js"><link rel="prefetch" href="/ranger-blog/assets/js/53.445eb867.js"><link rel="prefetch" href="/ranger-blog/assets/js/54.a27ff214.js"><link rel="prefetch" href="/ranger-blog/assets/js/55.b96718d3.js"><link rel="prefetch" href="/ranger-blog/assets/js/56.db852680.js"><link rel="prefetch" href="/ranger-blog/assets/js/57.3c3f9dde.js"><link rel="prefetch" href="/ranger-blog/assets/js/58.808fc746.js"><link rel="prefetch" href="/ranger-blog/assets/js/59.5b5aae5b.js"><link rel="prefetch" href="/ranger-blog/assets/js/6.c5327840.js"><link rel="prefetch" href="/ranger-blog/assets/js/60.261015e7.js"><link rel="prefetch" href="/ranger-blog/assets/js/61.9549b8ee.js"><link rel="prefetch" href="/ranger-blog/assets/js/62.c3ef0fd6.js"><link rel="prefetch" href="/ranger-blog/assets/js/63.86f2445a.js"><link rel="prefetch" href="/ranger-blog/assets/js/64.c1a7bdd7.js"><link rel="prefetch" href="/ranger-blog/assets/js/65.14eeb6d5.js"><link rel="prefetch" href="/ranger-blog/assets/js/66.f67c0b4b.js"><link rel="prefetch" href="/ranger-blog/assets/js/67.a9b212fa.js"><link rel="prefetch" href="/ranger-blog/assets/js/68.9c0b1d6b.js"><link rel="prefetch" href="/ranger-blog/assets/js/69.0d4e4e93.js"><link rel="prefetch" href="/ranger-blog/assets/js/7.f3f04205.js"><link rel="prefetch" href="/ranger-blog/assets/js/70.527f3db6.js"><link rel="prefetch" href="/ranger-blog/assets/js/71.cec9de49.js"><link rel="prefetch" href="/ranger-blog/assets/js/72.3bb3509b.js"><link rel="prefetch" href="/ranger-blog/assets/js/73.b92307f8.js"><link rel="prefetch" href="/ranger-blog/assets/js/74.3d52880c.js"><link rel="prefetch" href="/ranger-blog/assets/js/8.82de9728.js"><link rel="prefetch" href="/ranger-blog/assets/js/9.6f780161.js">
    <link rel="stylesheet" href="/ranger-blog/assets/css/0.styles.02a3a2c9.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/ranger-blog/" class="home-link router-link-active"><img src="/ranger-blog/img/logo.png" alt="游侠的博客" class="logo"> <span class="site-name can-hide">游侠的博客</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/ranger-blog/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="人工智能" class="dropdown-title"><a href="/ranger-blog/ai/" class="link-title">人工智能</a> <span class="title" style="display:none;">人工智能</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/ranger-blog/pages/ee7977/" aria-current="page" class="nav-link router-link-exact-active router-link-active">论文笔记</a></li><li class="dropdown-item"><h4>一些小知识点</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/ranger-blog/pages/52b4a3/" class="nav-link">pytorch、numpy、pandas函数简易解释</a></li></ul></li><li class="dropdown-item"><!----> <a href="/ranger-blog/note/dl500q/" class="nav-link">《深度学习500问》</a></li></ul></div></div><div class="nav-item"><a href="/ranger-blog/development/" class="nav-link">开发</a></div><div class="nav-item"><a href="/ranger-blog/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/ranger-blog/more/" class="nav-link">更多</a></div><div class="nav-item"><a href="/ranger-blog/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/ranger-blog/pages/2da524/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/ranger-blog/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/ranger-blog/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/ranger-blog/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/ranger-blog/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/747721653/ranger-blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.1co325on9eqo.webp"> <div class="blogger-info"><h3>Ranger</h3> <span>一名在校研究生</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/ranger-blog/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="人工智能" class="dropdown-title"><a href="/ranger-blog/ai/" class="link-title">人工智能</a> <span class="title" style="display:none;">人工智能</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/ranger-blog/pages/ee7977/" aria-current="page" class="nav-link router-link-exact-active router-link-active">论文笔记</a></li><li class="dropdown-item"><h4>一些小知识点</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/ranger-blog/pages/52b4a3/" class="nav-link">pytorch、numpy、pandas函数简易解释</a></li></ul></li><li class="dropdown-item"><!----> <a href="/ranger-blog/note/dl500q/" class="nav-link">《深度学习500问》</a></li></ul></div></div><div class="nav-item"><a href="/ranger-blog/development/" class="nav-link">开发</a></div><div class="nav-item"><a href="/ranger-blog/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/ranger-blog/more/" class="nav-link">更多</a></div><div class="nav-item"><a href="/ranger-blog/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/ranger-blog/pages/2da524/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/ranger-blog/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/ranger-blog/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/ranger-blog/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/ranger-blog/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/747721653/ranger-blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>论文笔记</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/ranger-blog/pages/ee7977/" aria-current="page" class="active sidebar-link">Transformer笔记</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/ranger-blog/pages/ee7977/#什么是transformer" class="sidebar-link">什么是Transformer</a></li><li class="sidebar-sub-header level2"><a href="/ranger-blog/pages/ee7977/#网络架构" class="sidebar-link">网络架构</a></li><li class="sidebar-sub-header level2"><a href="/ranger-blog/pages/ee7977/#模型的大体结构" class="sidebar-link">模型的大体结构</a></li></ul></li><li><a href="/ranger-blog/pages/e395d4/" class="sidebar-link">Gated Transformer Networks for Multivariate Time Series Classification</a></li><li><a href="/ranger-blog/pages/e9c758/" class="sidebar-link">SwingTransformer</a></li><li><a href="/ranger-blog/pages/45b6e8/" class="sidebar-link">Rocket、MiniRocket、MultiRocket</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>一些小知识点</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>《深度学习500问》</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>pytorch知识点</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/ranger-blog/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/ranger-blog/ai/#人工智能" data-v-06225672>人工智能</a></li><li data-v-06225672><a href="/ranger-blog/ai/#论文笔记" data-v-06225672>论文笔记</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/747721653" target="_blank" title="作者" class="beLink" data-v-06225672>yangzhixuan</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2023-02-15</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Transformer笔记<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h2 id="什么是transformer"><a href="#什么是transformer" class="header-anchor">#</a> 什么是Transformer</h2> <p>Transformer是一个基于<strong>纯注意力机制</strong>的网络模型，它没有运用循环或是卷积神经网络</p> <p>整体的结构为一个<strong>编码器-解码器</strong></p> <h2 id="网络架构"><a href="#网络架构" class="header-anchor">#</a> 网络架构</h2> <p><strong>编码器-解码器结构</strong>：</p> <p>编码器会将序列中的每一个词表示成一个单独的向量（输出）；
解码器拿到编码器的输出之后生成一个新的序列（和原始序列长度不一定相等），编码的时候可以一次性全部生成，
而解码的时候只能够一个个来（自回归（过去时刻的输出可以看做当前时刻的输入）），
解码第n个词的时候可以看到前n-1个词的信息，这里类似于rnn。</p> <div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image-20230215205919963.2ojlk7kw0wa0.webp" alt="image-20230215205919963"></p></div><p><code>Add&amp;Norm</code>：在每一个子层后面都加上一个这个，为了保证输入输出一致，这里设定每一个层输出的维度为512</p> <p><code>Norm</code>：每个子层的输出为：</p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="y"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="r"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="N"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="o"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="r"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-i"><mjx-c c="S"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="u"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="b"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="l"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="y"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="r"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container></p><p></p> <p><code>LayerNorm</code>：将单个样本（不同于<code>batchNorm</code>是对每个特征做正则化）内转换成均值为0，方差为1的格式（将每个值减去均值同时除以方差就行了）</p> <p><code>Add</code>：残差连接，作用有两个：</p> <ol><li><p>降低模型复杂度，放置过拟合</p></li> <li><p>防止梯度消失</p></li></ol> <p>没做残差连接的梯度<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo space="3" class="mjx-n"><mjx-c c="D7"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container>，网络层数很深之后容易导致梯度乘以接近0的数组从而让梯度消失。</p> <p>加了残差连接的梯度：<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo space="3" class="mjx-n"><mjx-c c="D7"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container>，加了一项且值相对较大，即使是很深的网络也不容易导致梯度消失</p> <p>解码器中的第一块<img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.5mis7o8t8oo0.webp" alt="https://cdn.staticaly.com/gh/747721653/image-store@master/image.5mis7o8t8oo0.webp" style="zoom:33%;">：带掩码的注意力机制，保证在t时间不会看到t时间之后的哪些输入</p> <p><strong>Attention</strong>：</p> <p>Attention的原理：</p> <div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.5w9bcygwij80.webp" alt="image"></p></div><p>首先输出是三个v的累加，假如query和第一个key比较接近，那么输出的时候第一个value的权重就会比较大，远离第一个value的value权重就会慢慢变小</p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class="MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="A"></mjx-c><mjx-c c="t"></mjx-c><mjx-c c="t"></mjx-c><mjx-c c="e"></mjx-c><mjx-c c="m"></mjx-c><mjx-c c="p"></mjx-c><mjx-c c="t"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="Q"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mi space="2" class="mjx-i"><mjx-c c="K"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mi space="2" class="mjx-i"><mjx-c c="V"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-n"><mjx-c c="s"></mjx-c><mjx-c c="o"></mjx-c><mjx-c c="f"></mjx-c><mjx-c c="t"></mjx-c><mjx-c c="m"></mjx-c><mjx-c c="a"></mjx-c><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mstyle><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c c="Q"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="K"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="T"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.037em;"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="k"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-mstyle><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="V"></mjx-c></mjx-mi></mjx-math></mjx-container></p><p></p> <p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="k"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container>指的是Q和K向量的维度，Q和K向量在编码器中的维度是一样的，长度可能不一，例如在解码器中由于Q是来自目标序列，因此不能保证Q和K的长度一致，向量的矩阵计算之后，做softmax就得到了对应的概率，相加为1</p> <p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.j9p19dwnz9s.webp" alt="image"><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.64p6ber6yo80.webp" alt="image"><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.1zxtei3umpk0.webp" alt="image"></p> <div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.aqonb4ofykc.webp" alt="image"></p></div><p>之所以要除以<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.037em;"><mjx-msub><mjx-TeXAtom><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi></mjx-TeXAtom><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="k"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container>，是为了防止向量做点积的时候<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="k"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container>很长时某些值过大导致softmax时概率更趋于1从而导致梯度变得很小，训练不动</p> <p>一整个的计算流程如下：</p> <div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.5ykt9ooth4c0.webp" alt="image"></p></div><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.2ecx2zr8lr6s.webp" style="zoom:50%;">这里的Mask是可选的，只有在解码器的时候有用到，用处是防止模型关注当前t时刻后面的信息，具体做法是给t时刻后面的这一块计算结果换成一个非常大的负数，那么在softmax中的指数计算就会趋近于0，其权重就会变成0，在之后与V的计算时就相当于是不参与计算了</p> <p><strong>多头注意力机制</strong>：</p> <div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.st8ia43n0ps.webp" alt="image"></p></div><p>之所以要使用多头注意力机制，是因为在原来单个注意力机制中，没有什么能够学习的参数，整个流程都是给定的，因此在多头注意力计算中，将V、K、Q做线性投影到低维，投影时给定的投影矩阵W是可以学习的，同时有h个这样的结构，相当于给了h次学习的机会，希望能够学到不一样的一些信息，最后拼接起来再投影回去</p> <div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.3m7sewtebjc0.webp" alt="image"></p></div><div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.6oyee92eink0.webp" alt="image"></p></div><p>这里h=8，<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mstyle><mjx-mspace style="width:0.222em;"></mjx-mspace></mjx-mstyle><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="k"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-TeXAtom><mjx-mi class="mjx-n"><mjx-c c="m"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c c="o"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c c="d"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c c="l"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-TeXAtom></mjx-script></mjx-msub><mjx-TeXAtom><mjx-mo class="mjx-n"><mjx-c c="/"></mjx-c></mjx-mo></mjx-TeXAtom><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-TeXAtom space="4"><mjx-mover><mjx-over style="padding-bottom:0.2em;padding-left:0.389em;"><mjx-TeXAtom size="s"></mjx-TeXAtom></mjx-over><mjx-base><mjx-TeXAtom><mjx-mo class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo></mjx-TeXAtom></mjx-base></mjx-mover></mjx-TeXAtom><mjx-mn space="4" class="mjx-n"><mjx-c c="6"></mjx-c><mjx-c c="4"></mjx-c></mjx-mn></mjx-math></mjx-container></p> <h2 id="模型的大体结构"><a href="#模型的大体结构" class="header-anchor">#</a> 模型的大体结构</h2> <div class="center-container"><p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.3qyixiugt3q0.webp" alt="image"></p></div><p>之所以是自注意力机制，是因为在进入多头注意力层的时候，同样的向量复制成了三份，分别作为K、Q、V。</p> <p>假定输入为长度为n的一个序列，向量化之后变为矩阵大小为n*d，输出大小也一致，输出的每一个向量都是输入的所有向量的加权累加（就是softmax那一步乘以V向量）</p> <p>解码器第一个子层的输出会被当做Q向量输入进第二个子层，其输入是另一个序列（机器翻译中的翻译结果）</p> <p>Transformer模型在训练和投入使用时的模式是不一样的，训练时由于知道目标输出序列，因此做mask后可以并行进行计算，但实际使用时是一个个单词输出的，每预测一次都会加大解码器输入序列的长度（开始的时候是0？）</p> <p><strong>Feed-forward（前馈神经网络）</strong>：</p> <p>具体公式描述：<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="F"></mjx-c><mjx-c c="F"></mjx-c><mjx-c c="N"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mo space="4" class="mjx-n"><mjx-c c="m"></mjx-c><mjx-c c="a"></mjx-c><mjx-c c="x"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mi space="2" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="b"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="b"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></p> <p>包含两个线性变换和一个Relu激活，transformer在这边对每个向量都会用相同的参数应用一次，之所以不对整个矩阵作用，是因为之前的Attention已经把每个向量的信息提取出来并aggregation了</p> <p>内层维度为<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="o"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="l"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub></mjx-math></mjx-container>的4倍，在这里就是2048，外层又映射回512</p> <p>空间复杂度为n*dff，dff = 4*<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="o"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="l"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub></mjx-math></mjx-container>，因为要保存每个向量的计算结果</p> <p><strong>Embedding</strong>：</p> <p>就是将词映射为一个向量，这里将权重乘了一个<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.037em;"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-TeXAtom><mjx-mtext class="mjx-mit"><mjx-c c="m"></mjx-c><mjx-c c="o"></mjx-c><mjx-c c="d"></mjx-c><mjx-c c="e"></mjx-c><mjx-c c="l"></mjx-c></mjx-mtext></mjx-TeXAtom></mjx-TeXAtom></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container></p> <p><strong>Positional Encoding</strong>：<strong>加入时序信息</strong></p> <p>首先将位置信息encoding，然后把它加进encoding后的原始数据中</p> <p>将词所在位置信息encoding进原始信息中</p> <p><img src="https://cdn.staticaly.com/gh/747721653/image-store@master/image.42w80q2xf500.webp" style="zoom:67%;">:第一个线性层是一个简单的全连接神经网络，它将解码器输出的向量投影到一个更大的向量中，相当于是单词表吧，每个向量的位置代表一个单词，之后经过一个softmax输出每个单词的概率，概率最高的作为模型的输出。</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/747721653/ranger-blog/edit/main/docs/01.人工智能/01.论文笔记/10.Transformer笔记.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="tags"><a href="/ranger-blog/tags/?tag=Transformer" title="标签">#Transformer</a><a href="/ranger-blog/tags/?tag=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="标签">#深度学习</a></div> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2023/04/24, 14:04:25</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><!----> <a href="/ranger-blog/pages/e395d4/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Gated Transformer Networks for Multivariate Time Series Classification</div></a></div> <div class="page-nav"><p class="inner"><!----> <span class="next"><a href="/ranger-blog/pages/e395d4/">Gated Transformer Networks for Multivariate Time Series Classification</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/ranger-blog/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/ranger-blog/pages/64be4a/"><div>
            七、RESTful风格
            <!----></div></a> <span class="date">04-24</span></dt></dl><dl><dd>02</dd> <dt><a href="/ranger-blog/pages/e2beba/"><div>
            六、SpringMVC的视图
            <!----></div></a> <span class="date">04-24</span></dt></dl><dl><dd>03</dd> <dt><a href="/ranger-blog/pages/f0c777/"><div>
            五、域对象共享数据
            <!----></div></a> <span class="date">04-23</span></dt></dl> <dl><dd></dd> <dt><a href="/ranger-blog/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:747721653@qq.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/747721653" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2023-2023
    <span>Ranger | <a href="https://github.com/747721653/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"></div></div>
    <script src="/ranger-blog/assets/js/app.cee5ce8e.js" defer></script><script src="/ranger-blog/assets/js/2.9c955742.js" defer></script><script src="/ranger-blog/assets/js/10.5d584cd8.js" defer></script>
  </body>
</html>
