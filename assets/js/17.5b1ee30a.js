(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{334:function(a,t,e){"use strict";e.r(t);var s=e(8),r=Object(s.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"clip【learning-transferable-visual-models-from-natural-language-supervision】"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#clip【learning-transferable-visual-models-from-natural-language-supervision】"}},[a._v("#")]),a._v(" CLIP【Learning transferable Visual Models From Natural Language Supervision】")]),a._v(" "),t("h3",{attrs:{id:"简单介绍"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#简单介绍"}},[a._v("#")]),a._v(" 简单介绍")]),a._v(" "),t("p",[a._v("通过图像文本对之间进行对比学习预训练网络，将预训练网络迁移到其他数据集做Zero-Shot的分类任务，有非常好的泛化性能与多模态能力")]),a._v(" "),t("p",[a._v("使用了大量的图像文本对（亿级）")]),a._v(" "),t("h3",{attrs:{id:"网络模型"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#网络模型"}},[a._v("#")]),a._v(" 网络模型")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/0e049dc8e41c0cdbcec21f539e506341.6bgvtpfdpp.webp",alt:"0e049dc8e41c0cdbcec21f539e506341"}})])]),t("ul",[t("li",[a._v("首先根据图像文本对计算相似度，以此计算对比损失预训练模型")]),a._v(" "),t("li",[a._v("迁移到其他数据集上：假如说1000类的ImageNet，使用一个文本prompt将类的文本填入{  }中，过文本编码器得到文本特征，将图像过图像编码器得到图像特征，计算相似度，之后过softmax将最大的作为预测类别")])]),a._v(" "),t("h1",{attrs:{id:"lseg【language-driven-semantic-segmentation】iclr-2022"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#lseg【language-driven-semantic-segmentation】iclr-2022"}},[a._v("#")]),a._v(" Lseg【Language-Driven Semantic Segmentation】ICLR 2022")]),a._v(" "),t("h3",{attrs:{id:"简单介绍-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#简单介绍-2"}},[a._v("#")]),a._v(" 简单介绍")]),a._v(" "),t("p",[a._v("将CLIP思想用于语义分割的一篇工作，没有用到对比学习，训练过程是有监督的训练")]),a._v(" "),t("h3",{attrs:{id:"实际效果"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#实际效果"}},[a._v("#")]),a._v(" 实际效果")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/4ca42c7bceb00eae0b434a79441cdbfa.6ik3p52frj.webp",alt:"4ca42c7bceb00eae0b434a79441cdbfa"}})])]),t("h3",{attrs:{id:"网络架构"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#网络架构"}},[a._v("#")]),a._v(" 网络架构")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/6d2b1fe43c0226f21ded3f59e0e0728f.6bgvtpgo90.webp",alt:"6d2b1fe43c0226f21ded3f59e0e0728f"}})])]),t("ul",[t("li",[a._v("给定一张图片与一组文本标签，建立图像与这组文本信息的对应关系")]),a._v(" "),t("li",[a._v("文本特征与图像特征相乘，通道从C变为N")]),a._v(" "),t("li",[a._v("先降维再升维")]),a._v(" "),t("li",[a._v("最后是有ground truch mask的，计算的损失就是在这个标签上计算的")])]),a._v(" "),t("h3",{attrs:{id:"评价"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#评价"}},[a._v("#")]),a._v(" 评价")]),a._v(" "),t("p",[a._v("思想很简单，只是将文本特征与图像特征乘了一下，这使得后面在使用这一模型的时候，可以使用本文作为prompt来分割图像")]),a._v(" "),t("h1",{attrs:{id:"group-vit【group-vit-semantic-segmentation-emerges-from-text-supervision】cvpr-2022"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#group-vit【group-vit-semantic-segmentation-emerges-from-text-supervision】cvpr-2022"}},[a._v("#")]),a._v(" Group ViT【Group ViT: Semantic Segmentation Emerges from Text Supervision】CVPR 2022")]),a._v(" "),t("h3",{attrs:{id:"简单介绍-3"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#简单介绍-3"}},[a._v("#")]),a._v(" 简单介绍")]),a._v(" "),t("p",[a._v("在语义分割中引入了Grouping Block，能够对图像中的像素点进行分组，解决了分割问题难以与一组文本计算对比损失的问题")]),a._v(" "),t("h3",{attrs:{id:"模型介绍"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#模型介绍"}},[a._v("#")]),a._v(" 模型介绍")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/3fae65e72d28261d86b8cf24b44239a4.9dcruxig12.webp",alt:"3fae65e72d28261d86b8cf24b44239a4"}})])]),t("ul",[t("li",[a._v("网络结构是ViT，在几个Transformer层中加入了两个Grouping Block层")]),a._v(" "),t("li",[a._v("第一个Grouping Block作用前先在前一个Transformer层上加入64个与特征一样大的token，可以理解为聚类中心")]),a._v(" "),t("li",[a._v("经Transformer层互相关注之后，在Grouping Block内部，经过几次矩阵乘法将特征的数量变为与之前加入的token数量，即64，这个过程可以看做是一个细分类")]),a._v(" "),t("li",[a._v("在Grouping Block内部，用到了gumbel softmax使这一过程变为可学习的，因为其中有一项矩阵乘法没有任何参数，无法学习")]),a._v(" "),t("li",[a._v("第二个Grouping Block与第一个中间隔了几层Transformer层，第二个与第一个计算差不多，不过减少了token的数量，以实现更宽泛的分类")]),a._v(" "),t("li",[a._v("最后将特征平均池化之后与文本特征计算对比损失")]),a._v(" "),t("li",[a._v("之后就与CLIP一样")])]),a._v(" "),t("p",[t("strong",[a._v("模型使用过程如下：")])]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/9c7b45c5a5ed527bfd0a8ae0df59d126.6bgvtphkg8.webp",alt:"9c7b45c5a5ed527bfd0a8ae0df59d126"}})])]),t("h3",{attrs:{id:"可视化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#可视化"}},[a._v("#")]),a._v(" 可视化")]),a._v(" "),t("p",[a._v("作者这里做了一个实验，将网络中途产生的特征单独拿出来进行分割，可以看到，第一个Grouping Block产生的特征关注的都是小物体，因为这里加入的token数量为64，比较多，而第二个Grouping Block就关注的是一些大物体了，因为这里的token数量为8")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/a500c6f514a5bb2e9f6799b599b923ef.2krq8gt3w6.webp",alt:"a500c6f514a5bb2e9f6799b599b923ef"}})])]),t("h1",{attrs:{id:"vild【open-vocabulary-object-detection-via-vision-and-language-knowledge-distillation】iclr-2022"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#vild【open-vocabulary-object-detection-via-vision-and-language-knowledge-distillation】iclr-2022"}},[a._v("#")]),a._v(" ViLD【OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION】ICLR 2022")]),a._v(" "),t("h3",{attrs:{id:"简单介绍-4"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#简单介绍-4"}},[a._v("#")]),a._v(" 简单介绍")]),a._v(" "),t("p",[a._v("这是一个做目标检测的网络，动机是能够检测到任意新的物品类别，在已有数据集的基础上去，训练过程为有监督的")]),a._v(" "),t("h3",{attrs:{id:"网络模型-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#网络模型-2"}},[a._v("#")]),a._v(" 网络模型")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/47e9e8d8303859f9c2b9839f01972739.13ll6ppgm1.webp",alt:"47e9e8d8303859f9c2b9839f01972739"}})])]),t("ul",[t("li",[a._v("a为baseline，就是一个maskRCNN")]),a._v(" "),t("li",[a._v("绿色的组件为可训练的，蓝色的表示不可训练，权重锁死的")]),a._v(" "),t("li",[a._v("b前面一部分就是正常的提取图像特征，back ground为背景类，与文本特征一起与图像特征计算相似度，之后计算损失")]),a._v(" "),t("li",[a._v("c为知识蒸馏的过程，这是为了让我们要训练网络的特征与预训练网络（CLIP）提取的特征尽可能的一致，这可以大大利用CLIP中的文本辨别力，这里的M pre-computed proposals是预先处理好的，为了减少计算量")]),a._v(" "),t("li",[a._v("d就是ViLD的整体架构")])]),a._v(" "),t("p",[t("strong",[a._v("更形象化的流程图：")])]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/c5d1c1df737261897c0991eb7e4b0520.8s348mpprm.webp",alt:"c5d1c1df737261897c0991eb7e4b0520"}})])]),t("h1",{attrs:{id:"glip【grounder-language-image-pre-training】"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#glip【grounder-language-image-pre-training】"}},[a._v("#")]),a._v(" GLIP【Grounder Language-Image Pre-training】")]),a._v(" "),t("h3",{attrs:{id:"简单介绍-5"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#简单介绍-5"}},[a._v("#")]),a._v(" 简单介绍")]),a._v(" "),t("p",[a._v("通过Vision Grounding来使用图像文本对训练模型")]),a._v(" "),t("p",[a._v("将检测与Grounding结合起来做模型的训练，分类损失与定位损失结合")]),a._v(" "),t("p",[a._v("有监督学习")]),a._v(" "),t("h3",{attrs:{id:"网络架构-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#网络架构-2"}},[a._v("#")]),a._v(" 网络架构")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/91bb3b98d6e82d6c27f9b4c1d0646154.7egl4lf5qr.webp",alt:"91bb3b98d6e82d6c27f9b4c1d0646154"}})])]),t("ul",[t("li",[a._v("根据bunding box拆分图像，利用图像编码器编码图像特征，利用文本编码器编码文本prompt的特征")]),a._v(" "),t("li",[a._v("得到特征之后多过了几层网络用于特征融合，以达到更好地效果")]),a._v(" "),t("li",[a._v("Alignment Loss就是图像文本对的相似损失，Localization Loss就是目标检测中的定位损失")])]),a._v(" "),t("h1",{attrs:{id:"clipasso-semantically-aware-object-sketching"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#clipasso-semantically-aware-object-sketching"}},[a._v("#")]),a._v(" CLIPasso：Semantically-Aware Object Sketching")]),a._v(" "),t("p",[a._v("保持语义信息的简笔素描")]),a._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/picx-images-hosting@master/paper/6debd1a9ff118b6353a397e241b3b337.1hs0xkz33v.webp",alt:"6debd1a9ff118b6353a397e241b3b337"}})])]),t("p",[a._v("抽象非常重要")]),a._v(" "),t("p",[a._v("网络前面几层的特征有更具体的空间信息（位置、几何形状、朝向等）")])])}),[],!1,null,null,null);t.default=r.exports}}]);