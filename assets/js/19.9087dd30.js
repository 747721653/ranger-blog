(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{335:function(e,a,t){"use strict";t.r(a);var s=t(8),r=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"_1-unsupervised-feature-learning-via-non-parametric-instance-discrimination"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-unsupervised-feature-learning-via-non-parametric-instance-discrimination"}},[e._v("#")]),e._v(" 1. Unsupervised Feature Learning via Non-Parametric Instance Discrimination")]),e._v(" "),a("h3",{attrs:{id:"动机"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#动机"}},[e._v("#")]),e._v(" 动机")]),e._v(" "),a("p",[e._v("在用一个已经训练好的有监督分类器对豹子的图片进行分类时，豹类图片的分数较高，而其他物品的分数很低，因此作者提出一种无监督学习方法——"),a("strong",[e._v("个体判别")]),e._v("，将每一个图片看成是一个类别，理想目标是将每一个图片区分开来。")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image.57fr66fpi2g0.webp",alt:"image"}})])]),a("h3",{attrs:{id:"方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法"}},[e._v("#")]),e._v(" 方法")]),e._v(" "),a("p",[e._v("首先使用CNN提取每个图像的特征，并形成一个Memory Bank字典，对每个训练样本，使用KNN，在字典中查找最相似的样本作为正样本，并随机挑选一些其它的样本作为负样本，在编码器进行梯度更新之后，对这次使用到的样本用新的编码器生成新的特征并更新Memory Bank字典")]),e._v(" "),a("p",[e._v("将选出的正负样本组合为一个分类问题，使用NCE损失来优化模型，具体而言，对于每个正样本，NPID会计算它与所有负样本之间的相似度得分，然后使用softmax函数将这些得分转换为概率分布。最终，使用交叉熵损失函数来最小化正样本和负样本之间的差异。")]),e._v(" "),a("p",[e._v("经过多次迭代之后，模型能够产生更加紧密的正样本聚类和更分散的负样本，从而得到更好的特征表示。")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image.3guem7sxptm0.webp",alt:"image"}})])]),a("h2",{attrs:{id:"_2-unsupervised-embedding-learning-via-invariant-and-spreading-instance-feature"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-unsupervised-embedding-learning-via-invariant-and-spreading-instance-feature"}},[e._v("#")]),e._v(" 2. Unsupervised Embedding Learning via Invariant and Spreading Instance Feature")]),e._v(" "),a("h3",{attrs:{id:"思想"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#思想"}},[e._v("#")]),e._v(" 思想")]),e._v(" "),a("p",[e._v("没有存储大量的负样本，训练的正负样本都来自同一个mini batch。")]),e._v(" "),a("h3",{attrs:{id:"方法-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法-2"}},[e._v("#")]),e._v(" 方法")]),e._v(" "),a("p",[e._v("在一个mini batch中，假如batch size为256，对这些图片做数据增强，之后将原图像作为正样本，其他所有的原图像与增强后的图像都作为负样本，这样一个batch中的正样本有256个，负样本有（256-1）*2个，使用一个编码器做端到端的训练。")]),e._v(" "),a("p",[e._v("但这篇论文的效果并不是特别好，因为在做对比学习的时候，负样本要尽可能的多")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image.65wawdepc480.webp",alt:"image"}})])]),a("h3",{attrs:{id:"读论文后总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#读论文后总结"}},[e._v("#")]),e._v(" 读论文后总结")]),e._v(" "),a("p",[a("strong",[e._v("主要思想：")])]),e._v(" "),a("p",[e._v("通过数据增强构造正样本，通过CNN网络提取图像特征，并构造了一个负对数似然损失，通过最小化该损失令相似图像在特征空间中靠近，其他图像特征远离，并使用knn验证最后结果。")]),e._v(" "),a("p",[a("strong",[e._v("创新点：")])]),e._v(" "),a("p",[e._v("减少了计算量，不需要使用所有的负样本，而是每一个batch中的数据自己与自己比较")]),e._v(" "),a("p",[a("strong",[e._v("如何应用自己实验：")])]),e._v(" "),a("p",[e._v("增强----\x3e差分、趋势项、季节项等")]),e._v(" "),a("p",[e._v("网络结构更换----\x3eTransformer、一维卷积")]),e._v(" "),a("p",[e._v("总之就是想方法提取数据中的特征之后使用KNN等方法比较")]),e._v(" "),a("h2",{attrs:{id:"_3-representation-learning-with-contrastive-predictive-coding-可以用于序列任务"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-representation-learning-with-contrastive-predictive-coding-可以用于序列任务"}},[e._v("#")]),e._v(" 3. Representation Learning with Contrastive Predictive Coding（可以用于序列任务）")]),e._v(" "),a("p",[e._v("用预测去做对比学习")]),e._v(" "),a("p",[e._v("非常灵活，换成单词或图片中的patch都可以")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image.6woyd62za980.webp",alt:"image"}})])]),a("h2",{attrs:{id:"_4-contrastive-multiview-coding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-contrastive-multiview-coding"}},[e._v("#")]),e._v(" 4. Contrastive Multiview Coding")]),e._v(" "),a("h3",{attrs:{id:"思想-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#思想-2"}},[e._v("#")]),e._v(" 思想")]),e._v(" "),a("p",[e._v("一个物体的很多个视角都能作为正样本（相当于不同的观察方式：深度、分割等）")]),e._v(" "),a("p",[e._v("增大视角之间的互信息")]),e._v(" "),a("p",[e._v("如何区分正负样本：同一个图片的不同视角都可以作为正样本")]),e._v(" "),a("p",[e._v("局限：在处理不同视角或不同模态数据时，可能需要不同的编码器")]),e._v(" "),a("h3",{attrs:{id:"读论文笔记与思考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#读论文笔记与思考"}},[e._v("#")]),e._v(" 读论文笔记与思考")]),e._v(" "),a("p",[e._v("该论文的思想是学习图像不同视角的特征表示，以同一输入的不同视角互为正样本，不同输入的不同视角为负样本，作者根据这一特点设计了多个编码器，对不同视角的图像单独进行编码，这些编码器可以共享参数或独立训练，此外，作者还设计了一个对比学习损失函数，对于每个正样本对，通过计算两个视图的特征向量之间的相似度来确定其距离，而对于每个负样本对，则通过计算相同视图中的两个随机选择的特征向量之间的相似度来确定其距离。在模型学习时，就是通过最小化正样本对之间的距离和最大化负样本对之间的距离来优化特征表示。")]),e._v(" "),a("p",[e._v("问题：损失函数中是否为正样本之间的距离设置权重，因为如果出现正样本之间距离稍远但负样本之间距离比较大的情况，模型是不是就忽略了正样本之间的距离问题")]),e._v(" "),a("h2",{attrs:{id:"_5-momentum-contrast-for-unsupervised-visual-representation-learning-moco"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-momentum-contrast-for-unsupervised-visual-representation-learning-moco"}},[e._v("#")]),e._v(" 5. Momentum Contrast for Unsupervised Visual Representation Learning（MoCo）")]),e._v(" "),a("h3",{attrs:{id:"贡献"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#贡献"}},[e._v("#")]),e._v(" 贡献")]),e._v(" "),a("p",[e._v("将之前很多对比学习的工作都归纳成了一个字典查询的问题，提出了队列和动量编码器，从而去形成一个又大又一致的字典帮助更好的对比学习")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image.5l23sqlu21k0.webp",alt:"image"}})])]),a("h2",{attrs:{id:"_6-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr"}},[e._v("#")]),e._v(" 6. A Simple Framework for Contrastive Learning of Visual Representations（SimCLR)")]),e._v(" "),a("h3",{attrs:{id:"贡献-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#贡献-2"}},[e._v("#")]),e._v(" 贡献")]),e._v(" "),a("p",[e._v("和2论文思想差不多，但在得到特征之后加入了一个mlp层，进行了一次非线性变换，简单的结构却得到了很好的效果")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image.739d77t8qlo0.webp",alt:"image"}})])]),a("h2",{attrs:{id:"_7-improved-baselines-with-momentum-contrastive-learning-moco-v2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-improved-baselines-with-momentum-contrastive-learning-moco-v2"}},[e._v("#")]),e._v(" 7. Improved Baselines with Momentum Contrastive Learning（MoCo v2）")]),e._v(" "),a("p",[e._v("将SimCLR的一些改进放到了MoCo上")]),e._v(" "),a("h2",{attrs:{id:"_8-big-self-supervised-models-are-strong-semi-supervised-learners-simclr-v2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-big-self-supervised-models-are-strong-semi-supervised-learners-simclr-v2"}},[e._v("#")]),e._v(" 8. Big Self-Supervised Models are Strong Semi-Supervised Learners（SimCLR V2）")]),e._v(" "),a("h3",{attrs:{id:"改进"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#改进"}},[e._v("#")]),e._v(" 改进")]),e._v(" "),a("ol",[a("li",[e._v("使用了更大的模型")]),e._v(" "),a("li",[e._v("mlp层数加深")]),e._v(" "),a("li",[e._v("也使用了动量编码器")])]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image-20230418214956780.59o770niq9g0.webp",alt:"image-20230418214956780"}})])]),a("h2",{attrs:{id:"_9-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments"}},[e._v("#")]),e._v(" 9. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments")]),e._v(" "),a("p",[e._v("把对比学习和聚类的方法进行结合，相似的物体靠近，不相近的远离")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image-20230418220801403.3s14ggg32u00.webp",alt:"image-20230418220801403"}})])]),a("p",[e._v("其它参考论文：deep cluster、deep cluster2")]),e._v(" "),a("p",[e._v("multi crop")]),e._v(" "),a("h2",{attrs:{id:"_10-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_10-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning"}},[e._v("#")]),e._v(" 10. Bootstrap Your Own Latent A New Approach to Self-Supervised Learning")]),e._v(" "),a("p",[e._v("不使用负样本进行对比学习")]),e._v(" "),a("p",[e._v("自己预测自己")]),e._v(" "),a("p",[e._v("用自己一个视角的特征去预测另一个视角的特征")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image-20230418222414977.1tuc9bxj4xa8.webp",alt:"image-20230418222414977"}})])]),a("h3",{attrs:{id:"探究"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#探究"}},[e._v("#")]),e._v(" 探究")]),e._v(" "),a("h4",{attrs:{id:"可能原因1"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#可能原因1"}},[e._v("#")]),e._v(" 可能原因1")]),e._v(" "),a("p",[e._v("BYOL不使用负样本训练不坍塌是使用了BatchNorm。解释：BatchNorm在计算的时候能够看到其它样本的部分特征，存在一定的信息泄露")]),e._v(" "),a("p",[e._v("正样本图片与平均图片有什么差别")]),e._v(" "),a("h4",{attrs:{id:"可能原因2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#可能原因2"}},[e._v("#")]),e._v(" 可能原因2")]),e._v(" "),a("p",[e._v("模型需要比较好的初始化")]),e._v(" "),a("p",[e._v("可以去看 BYOL works even without batch statistics 这篇论文")]),e._v(" "),a("h2",{attrs:{id:"_11-exploring-simple-siamese-representation-learning-simsiam"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_11-exploring-simple-siamese-representation-learning-simsiam"}},[e._v("#")]),e._v(" 11. Exploring Simple Siamese Representation Learning（SimSiam）")]),e._v(" "),a("h3",{attrs:{id:"特点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#特点"}},[e._v("#")]),e._v(" 特点")]),e._v(" "),a("ol",[a("li",[e._v("不需要负样本")]),e._v(" "),a("li",[e._v("不需要很大的batch size")]),e._v(" "),a("li",[e._v("不需要动量编码器")])]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image-20230418224314812.1nbelf92350g.webp",alt:"image-20230418224314812"}})])]),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image-20230418224640276.26vtepjm11c0.webp",alt:"image-20230418224640276"}})])]),a("h2",{attrs:{id:"_12-an-empirical-study-of-training-self-supervised-vision-transformers-moco-v3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-an-empirical-study-of-training-self-supervised-vision-transformers-moco-v3"}},[e._v("#")]),e._v(" 12. An Empirical Study of Training Self-Supervised Vision Transformers(MoCo V3)")]),e._v(" "),a("h2",{attrs:{id:"_13-emerging-properties-in-self-supervised-vision-transformers-dino"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_13-emerging-properties-in-self-supervised-vision-transformers-dino"}},[e._v("#")]),e._v(" 13. Emerging Properties in Self-Supervised Vision Transformers(DINO)")]),e._v(" "),a("h1",{attrs:{id:"总览"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总览"}},[e._v("#")]),e._v(" 总览")]),e._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/747721653/image-store@master/paper/image-20230419104904659.4fvmqdhk0fe0.webp",alt:"image-20230419104904659"}})])])])}),[],!1,null,null,null);a.default=r.exports}}]);