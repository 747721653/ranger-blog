(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{342:function(t,a,s){"use strict";s.r(a);var i=s(8),c=Object(i.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"线性判别分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#线性判别分析"}},[t._v("#")]),t._v(" 线性判别分析")]),t._v(" "),a("p",[t._v("线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。")]),t._v(" "),a("p",[t._v("LDA分类思想简单总结如下：")]),t._v(" "),a("ol",[a("li",[t._v("多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。")]),t._v(" "),a("li",[t._v("对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。")]),t._v(" "),a("li",[t._v("对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。")]),t._v(" "),a("li",[t._v("如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。")])]),t._v(" "),a("h3",{attrs:{id:"图解lda核心思想"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#图解lda核心思想"}},[t._v("#")]),t._v(" 图解LDA核心思想")]),t._v(" "),a("p",[t._v("假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。")]),t._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.statically.io/gh/747721653/picx-images-hosting@master/tips/image.1g6bdcimor6o.jpg",alt:"image"}})])]),a("p",[t._v("左图和右图是两种不同的投影方式。")]),t._v(" "),a("p",[t._v("左图思路：让不同类别的平均点距离最远的投影方式。")]),t._v(" "),a("p",[t._v("右图思路：让同类别的数据挨得最近的投影方式。")]),t._v(" "),a("p",[t._v("从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。")]),t._v(" "),a("p",[t._v("以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。")]),t._v(" "),a("h3",{attrs:{id:"lda优缺点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lda优缺点"}},[t._v("#")]),t._v(" LDA优缺点")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("优缺点")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("简要说明")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("优点")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("1. 可以使用类别的先验知识；"),a("br"),t._v("2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("缺点")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("1. LDA不适合对非高斯分布样本进行降维；"),a("br"),t._v("2. LDA降维最多降到分类数k-1维；"),a("br"),t._v("3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；"),a("br"),t._v("4. LDA可能过度拟合数据。")])])])]),t._v(" "),a("h2",{attrs:{id:"主成分分析pca"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#主成分分析pca"}},[t._v("#")]),t._v(" 主成分分析PCA")]),t._v(" "),a("ol",[a("li",[t._v("PCA就是将高维的数据通过线性变换投影到低维空间上去。")]),t._v(" "),a("li",[t._v("投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。")]),t._v(" "),a("li",[t._v("去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。")]),t._v(" "),a("li",[t._v("去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。")]),t._v(" "),a("li",[t._v("对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。")]),t._v(" "),a("li",[t._v("完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。")]),t._v(" "),a("li",[t._v("之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。")])]),t._v(" "),a("h3",{attrs:{id:"图解pca核心思想"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#图解pca核心思想"}},[t._v("#")]),t._v(" 图解PCA核心思想")]),t._v(" "),a("p",[t._v("PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n < m），这n维形成主元，是重构出来最能代表原始数据的正交特征。")]),t._v(" "),a("p",[t._v("假设数据集是m个n维，"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-msup",[a("mjx-mi",{staticClass:"mjx-b mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-msup",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-b mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"2"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"22EF"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-msup",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-b mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("。如果"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"2"}})],1)],1)],1),t._v("，需要降维到"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msup",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[a("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"2032"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v("，现在想找到某一维度方向代表这两个维度的数据。下图有"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"u"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-msub",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"u"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1),t._v("两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？")],1),t._v(" "),a("div",{staticClass:"center-container"},[a("p",[a("img",{attrs:{src:"https://cdn.statically.io/gh/747721653/picx-images-hosting@master/tips/image.ezi10u8ehv4.jpg",alt:"image"}})])]),a("p",[t._v("从图可看出，"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"u"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v("比"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"u"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1),t._v("好，为什么呢？有以下两个主要评价指标：")],1),t._v(" "),a("ol",[a("li",[t._v("样本点到这个直线的距离足够近。")]),t._v(" "),a("li",[t._v("样本点在这个直线上的投影能尽可能的分开。")])]),t._v(" "),a("p",[t._v("如果我们需要降维的目标维数是其他任意维，则：")]),t._v(" "),a("ol",[a("li",[t._v("样本点到这个超平面的距离足够近。")]),t._v(" "),a("li",[t._v("样本点在这个超平面上的投影能尽可能的分开。")])]),t._v(" "),a("h3",{attrs:{id:"pca算法主要优缺点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pca算法主要优缺点"}},[t._v("#")]),t._v(" PCA算法主要优缺点")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("优缺点")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("简要说明")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("优点")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。3. 计算方法简单，主要运算是特征值分解，易于实现。")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("缺点")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。")])])])]),t._v(" "),a("h2",{attrs:{id:"lda和pca区别"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lda和pca区别"}},[t._v("#")]),t._v(" LDA和PCA区别")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("异同点")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("LDA")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("PCA")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("相同点")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("1. 两者均可以对数据进行降维；"),a("br"),t._v("2. 两者在降维时均使用了矩阵特征分解的思想；"),a("br"),t._v("3. 两者都假设数据符合高斯分布；")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}})]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("不同点")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("有监督的降维方法；")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("无监督的降维方法；")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}}),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("降维最多降到k-1维；")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("降维多少没有限制；")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}}),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("可以用于降维，还可以用于分类；")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("只用于降维；")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}}),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("选择分类性能最好的投影方向；")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("选择样本点投影具有最大方差的方向；")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}}),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("更明确，更能反映样本间差异；")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("目的较为模糊；")])])])]),t._v(" "),a("h2",{attrs:{id:"降维的必要性及目的"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#降维的必要性及目的"}},[t._v("#")]),t._v(" 降维的必要性及目的")]),t._v(" "),a("p",[a("strong",[t._v("降维的必要性")]),t._v("：")]),t._v(" "),a("ol",[a("li",[t._v("多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。")]),t._v(" "),a("li",[t._v("高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。")]),t._v(" "),a("li",[t._v("过多的变量，对查找规律造成冗余麻烦。")]),t._v(" "),a("li",[t._v("仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。")])]),t._v(" "),a("p",[a("strong",[t._v("降维的目的")]),t._v("：")]),t._v(" "),a("ol",[a("li",[t._v("减少预测变量的个数。")]),t._v(" "),a("li",[t._v("确保这些变量是相互独立的。")]),t._v(" "),a("li",[t._v("提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。")]),t._v(" "),a("li",[t._v("数据在低维下更容易处理、更容易使用。")]),t._v(" "),a("li",[t._v("去除数据噪声。")]),t._v(" "),a("li",[t._v("降低算法运算开销。")])])])}),[],!1,null,null,null);a.default=c.exports}}]);