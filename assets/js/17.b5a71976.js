(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{334:function(t,s,r){"use strict";r.r(s);var v=r(7),_=Object(v.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"生成模型和判别模型的区别"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#生成模型和判别模型的区别"}},[t._v("#")]),t._v(" 生成模型和判别模型的区别")]),t._v(" "),s("p",[s("strong",[t._v("生成模型")]),t._v("：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)（贝叶斯概率）。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。典型的生成模型有朴素贝叶斯，隐马尔科夫模型等")]),t._v(" "),s("p",[s("strong",[t._v("判别模型")]),t._v("：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。这些模型的特点都是输入属性X可以直接得到后验概率P(Y|X)，输出条件概率最大的作为最终的类别（对于二分类任务来说，实际得到一个score，当score大于threshold时则为正类，否则为负类）。")]),t._v(" "),s("h3",{attrs:{id:"举例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#举例"}},[t._v("#")]),t._v(" 举例")]),t._v(" "),s("p",[s("strong",[t._v("判别式模型举例")]),t._v("：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。")]),t._v(" "),s("p",[s("strong",[t._v("生成式模型举例")]),t._v("：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。")]),t._v(" "),s("h3",{attrs:{id:"联系和区别"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#联系和区别"}},[t._v("#")]),t._v(" 联系和区别")]),t._v(" "),s("p",[s("strong",[t._v("生成方法的特点")]),t._v("：上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y,X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。")]),t._v(" "),s("p",[s("strong",[t._v("判别方法的特点")]),t._v("：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。")]),t._v(" "),s("p",[t._v("最后，由生成模型可以得到判别模型，但由判别模型得不到生成模型。")])])}),[],!1,null,null,null);s.default=_.exports}}]);