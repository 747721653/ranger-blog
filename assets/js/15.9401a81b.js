(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{332:function(t,r,a){"use strict";a.r(r);var s=a(8),e=Object(s.a)({},(function(){var t=this,r=t._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h2",{attrs:{id:"说明"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#说明"}},[t._v("#")]),t._v(" 说明")]),t._v(" "),r("p",[r("strong",[t._v("作用")]),t._v("：避免随着网络深度加深导致的样本分布改变的问题，且保持在均值为0、方差为1的情况能够比较好地避免梯度消失")]),t._v(" "),r("p",[t._v("参考："),r("a",{attrs:{href:"https://blog.csdn.net/Sciws/article/details/126701282",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://blog.csdn.net/Sciws/article/details/126701282"),r("OutboundLink")],1)]),t._v(" "),r("p",[r("strong",[t._v("BatchNorm")]),t._v("：对一个batch-size样本内的每个特征做归一化")]),t._v(" "),r("p",[r("strong",[t._v("LayerNorm")]),t._v("：针对每条样本，对每条样本的所有特征做归一化")]),t._v(" "),r("h2",{attrs:{id:"举例"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#举例"}},[t._v("#")]),t._v(" 举例")]),t._v(" "),r("p",[t._v("假设现在有个二维矩阵，行代表batch-size，列代表样本特征")]),t._v(" "),r("ul",[r("li",[t._v("BatchNorm就是对这个二维矩阵中每一列的特征做归一化，即竖着做归一化")]),t._v(" "),r("li",[t._v("LayerNorm就是对这个二维矩阵中每一行数据做归一化，即横着做归一化")])]),t._v(" "),r("h2",{attrs:{id:"异同点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#异同点"}},[t._v("#")]),t._v(" 异同点")]),t._v(" "),r("h3",{attrs:{id:"相同点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#相同点"}},[t._v("#")]),t._v(" 相同点")]),t._v(" "),r("p",[r("strong",[t._v("都是在深度学习中让当前层的参数稳定下来，避免梯度消失或者梯度爆炸，方便后面的继续学习")])]),t._v(" "),r("h3",{attrs:{id:"不同点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#不同点"}},[t._v("#")]),t._v(" 不同点")]),t._v(" "),r("ul",[r("li",[t._v("如果你的特征依赖不同样本的统计参数，那BatchNorm更有效， 因为它不考虑不同特征之间的大小关系，但是保留不同样本间的大小关系")]),t._v(" "),r("li",[t._v("Nlp领域适合用LayerNorm， CV适合BatchNorm")]),t._v(" "),r("li",[t._v("对于Nlp来说，它不考虑不同样本间的大小关系，保留样本内不同特征之间的大小关系")])])])}),[],!1,null,null,null);r.default=e.exports}}]);