---
title: BatchNorm和LayerNorm的区别
date: 2023-02-16 20:42:32
permalink: /pages/08ee34/
categories:
  - 人工智能
  - 一些小知识点
tags:
  - 
author: 
  name: yangzhixuan
  link: https://github.com/747721653
---
## 说明

**BatchNorm**：对一个batch-size样本内的每个特征做归一化

**LayerNorm**：针对每条样本，对每条样本的所有特征做归一化

## 举例

假设现在有个二维矩阵，行代表batch-size，列代表样本特征

* BatchNorm就是对这个二维矩阵中每一列的特征做归一化，即竖着做归一化
* LayerNorm就是对这个二维矩阵中每一行数据做归一化，即横着做归一化

## 异同点

### 相同点

**都是在深度学习中让当前层的参数稳定下来，避免梯度消失或者梯度爆炸，方便后面的继续学习**

### 不同点

* 如果你的特征依赖不同样本的统计参数，那BatchNorm更有效， 因为它不考虑不同特征之间的大小关系，但是保留不同样本间的大小关系 
* Nlp领域适合用LayerNorm， CV适合BatchNorm
* 对于Nlp来说，它不考虑不同样本间的大小关系，保留样本内不同特征之间的大小关系

