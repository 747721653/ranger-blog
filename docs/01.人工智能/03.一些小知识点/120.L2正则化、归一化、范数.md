---
title: L2正则化、归一化、范数
date: 2024-04-09 20:54:05
permalink: /pages/97f411/
categories:
  - 人工智能
  - 一些小知识点
tags:
  - 
author: 
  name: yangzhixuan
  link: https://github.com/747721653
---


## L2范数
L2范数是指向量中元素的平方和的平方根，也称为欧几里得范数。对于一个包含$n$个元素的向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]$，其L2范数可以表示为：

$$\|\mathbf{x}\|_2=\sqrt{x_1^2+x_2^2+\ldots+x_n^2}$$


## L2正则化（Regularization）
L2正则化是指在损失函数中添加一个用于惩罚模型权重的项，以减少模型的复杂度，防止过拟合。

L2正则化通常用于神经网络和其他机器学习模型中，可以写成如下形式：

$$L_{\text{L}2}=L+\frac12\lambda\sum_{i=1}^nw_i^2$$

其中，$L$是原始的损失函数，$\lambda$是正则化参数，$n$是模型中的权重数量，$w_{i}$是第$i$个权重参数。


## L2范数与L2正则化之间的关系
L2正则化中的正则化项就是指的L2范数的平方，即$\sum_{i=1}^nw_i^2$。因此，L2正则化可以被视作对模型参数进行L2范数惩罚的一种形式。

## L2归一化（Normalization）
L2归一化是一种常见的数据处理技术，通常用于对向量进行标准化或归一化操作。它的目的是将向量的每个元素除以该向量的L2范数，使得整个向量的长度为1。

假设有一个包含$n$个元素的向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]$，则L2归一化的过程如下：

1. 计算向量$x$的L2范数： 

    $$\|\mathbf{x}\|_2=\sqrt{x_1^2+x_2^2+\ldots+x_n^2}$$

2. 将向量$x$的每个元素除以L2范数，得到L2归一化后的向量$\mathbf{x}_{\mathrm{norm}}$：

    $$\mathbf{x}_{\mathrm{norm}}=\frac{\mathbf{x}}{\|\mathbf{x}\|_2}=\left[\frac{x_1}{\|\mathbf{x}\|_2},\frac{x_2}{\|\mathbf{x}\|_2},\ldots,\frac{x_n}{\|\mathbf{x}\|_2}\right]$$

    经过L2归一化处理后，向量$\mathbf{x}_{\mathrm{norm}}$的L2范数（长度）将等于1，即$\|\mathbf{x}_\text{norm}\|_2=1$，因此它也被称为单位向量或规范化向量。

    补充：两个归一化后的向量相乘相当于是计算它们之间的余弦相似度

## 其他补充
### 向量点积与外积（叉积）
1. **点积（内积）**：点积是指两个向量对应元素相乘后再求和的操作，得到的结果是一个标量（数量），而不是向量。假设有两个向量$\mathbf{a}=[a_1,a_2,a_3,\ldots,a_n]$和$\mathbf{b}=[b_1,b_2,b_3,\ldots,b_n]$，它们的点积计算公式为：

    $$\mathbf{a}\cdot\mathbf{b}=\sum_{i=1}^na_i\cdot b_i$$
    
    这个公式表示将$a$和$b$对应位置的元素相乘，然后将结果相加得到最终的标量值。
    
    当向量跟自身计算点积时相当于计算自身L2范数的平方。如有一个向量$\mathbf{v}=[v_1,v_2,v_3,\ldots,v_n]$，其对自身的点积计算公式为：

    $$\mathbf{v}\cdot\mathbf{v}=\sum_{i=1}^nv_i^2$$

2. **外积（叉积）**：外积是指两个向量的叉乘操作，结果是一个新的向量，这个向量与原来的两个向量都垂直。外积通常用于三维空间中，对于两个三维向量$\mathbf{a}=[a_1,a_2,a_3]$和$\mathbf{b}=[b_1,b_2,b_3]$，它们的外积计算公式为：

    $$\mathbf{a}\times\mathbf{b}=[a_2b_3-a_3b_2,a_3b_1-a_1b_3,a_1b_2-a_2b_1]$$

    这个公式表示按照特定顺序将$a$和$b$的对应元素进行计算得到新向量的每个分量值。

