---
title: SwingTransformer
date: 2023-03-05 10:55:34
permalink: /pages/e9c758/
categories:
  - 人工智能
  - 论文笔记
tags:
  - 
author: 
  name: yangzhixuan
  link: https://github.com/747721653
---

## Transformer与Vision Transformer对比
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.1m5y9u5yh4e8.jpg)
:::

**不同**：

1. Swin Transformer构建的feature map是具有层次性的，网络越深下采样倍率越大
2. 红框为窗口。Swin Transformer的窗口是分开的，VIT是连起来的，这样做能够大大减低运算量

## 整体框架图
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.7fu6ucpymsg.jpg)
:::

**Patch Partition**：
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.ipetr4jsxtc.jpg)
:::
将图片的每一个窗口，将它的patch在channel方向进行展平

整体框架图中经过Patch Partition后的48就是RGB三通道数*展平后的16块即3*16得到的

之后，通过Linear Embeding层对每个像素的channel数据做线性变换，由48变成C


## Patch Merging
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.6qee0098we00.jpg)
:::

每个窗口中的patch被分为四类，将相同位置的patch放置在一起，在通道方向进行连接，并对每一个通道分别进行LayerNorm，最后经过一个线性层将通道数减半，最后整体的效果就是feature map的通道数翻倍，高和宽减半

## W-MSA
全称：窗口-多头注意力

目的：减少计算量

缺点：窗口之间无法进行信息交互

:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.14sluuy3do8w.jpg)
:::

原MSA：图片上的每一个像素对其他所有像素做self attention

W-MSA：每个窗口内部做self attention

节省的计算量：
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.4h89tcdfslu0.jpg)
:::

## Shifted Window
目的：实现不同window之间的信息交互，之前情况中窗口与窗口之间是没有通讯的

:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.4q2pdz9vw7i0.jpg)
:::

变换之后的窗口能够计算不同之前不同窗口之间的自注意力，但如果直接进行计算的话，将需要计算9个窗口的自注意力，因此作者采用了另外一种方法

原分割情况：
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.3amli47e7a00.jpg)
:::

窗口转换之后结果：
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.31gckcwzjj20.jpg)
:::

具体计算图（以5、3区域为例）：
:::center
![image](https://cdn.statically.io/gh/747721653/picx-images-hosting@master/paper/image.gmoe99jr6gw.jpg)
:::
计算注意力的时候依旧是对一整块区域计算注意力，但区域5和区域3之间交叉的地方需要抛弃，这里采用的方法是在给定位置一个比较小的数，这样做softmax就会变成0，也就消失了

## 参考资料
[沐神视频](https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.788&vd_source=6a9e9f8459576ecbca10411d0f2a4c8a)

[霹雳吧啦视频](https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.999.0.0&vd_source=6a9e9f8459576ecbca10411d0f2a4c8a)

[博客](https://blog.csdn.net/qq_37541097/article/details/121119988)







